# 🚀 AI Interview Simulator

A voice-enabled mock interview platform where an AI conducts interviews, analyzes responses in real-time, and generates dynamic follow-up questions.

## ✨ Features

- **🎤 Voice Interaction**: Real-time speech recognition using browser APIs
- **🧠 AI-Powered Analysis**: 
  - Confidence scoring via sentiment analysis
  - Clarity assessment through grammar and structure analysis
  - Relevance scoring using semantic similarity
- **📊 Real-time Feedback**: Instant visual feedback with detailed scores
- **🔄 Dynamic Questions**: Context-aware follow-up questions generated by AI
- **📱 Responsive Design**: Works on desktop and mobile devices

## 🛠️ Tech Stack

### Frontend
- **React 18** - Modern UI framework
- **react-speech-recognition** - Browser-based voice recognition
- **Axios** - HTTP client for API communication
- **CSS3** - Modern styling with gradients and animations

### Backend
- **FastAPI** - High-performance Python web framework
- **Whisper** - OpenAI's speech-to-text model
- **BERT/RoBERTa** - Sentiment analysis for confidence scoring
- **Sentence Transformers** - Semantic similarity for relevance
- **NLTK** - Natural language processing for clarity analysis
- **OpenAI GPT** - Dynamic question generation

## 🚀 Quick Start

### Prerequisites

- Python 3.8+
- Node.js 16+
- npm or yarn
- OpenAI API key (optional, for enhanced question generation)

### Backend Setup

1. **Navigate to backend directory**
   ```bash
   cd ai-interview-simulator/backend
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env and add your OpenAI API key (optional)
   ```

5. **Start the backend server**
   ```bash
   python main.py
   ```
   
   The API will be available at `http://localhost:8000`

### Frontend Setup

1. **Navigate to frontend directory**
   ```bash
   cd ai-interview-simulator/frontend
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Start the development server**
   ```bash
   npm start
   ```
   
   The app will open at `http://localhost:3000`

## 📖 Usage Guide

### Starting an Interview

1. **Open the application** in your browser
2. **Click "Start Interview"** to begin
3. **Allow microphone access** when prompted
4. **Read the first question** displayed on screen

### Recording Your Response

1. **Click "🎤 Start Recording"** to begin speaking
2. **Speak clearly** into your microphone
3. **Watch the live transcript** appear as you speak
4. **Click "⏹️ Stop & Analyze"** when finished

### Understanding Feedback

The AI provides three key metrics:

- **Confidence (0-100%)**: Based on sentiment analysis and language patterns
- **Clarity (0-100%)**: Grammar, structure, and filler word analysis
- **Relevance (0-100%)**: How well your answer addresses the question

### Continuing the Interview

- **Review the feedback** and suggestions
- **Read the next question** generated based on your response
- **Continue the cycle** of recording and receiving feedback
- **Use "🔄 Reset Interview"** to start over anytime

## 🔧 API Endpoints

### Backend API

- `GET /` - Health check
- `POST /start-interview` - Initialize interview session
- `POST /analyze-response` - Analyze audio response (with file upload)
- `POST /text-analysis` - Analyze text response (for testing)

### Example API Usage

```python
import requests

# Start interview
response = requests.post("http://localhost:8000/start-interview")
question = response.json()["question"]

# Analyze text response
response = requests.post(
    "http://localhost:8000/text-analysis",
    params={
        "transcript": "I am a software engineer with 5 years of experience...",
        "question": "Tell me about yourself"
    }
)
analysis = response.json()["analysis"]
```

## 🎯 Key Components

### AI Analysis Pipeline

1. **Speech-to-Text**: Browser API → Whisper (fallback)
2. **Confidence Analysis**: RoBERTa sentiment model + linguistic patterns
3. **Clarity Analysis**: NLTK grammar analysis + filler word detection
4. **Relevance Analysis**: Sentence-BERT semantic similarity
5. **Question Generation**: GPT-3.5/4 with context-aware prompts

### Frontend Architecture

```
src/
├── App.js          # Main application component
├── App.css         # Styling and animations
├── index.js        # React app entry point
└── index.css       # Global styles
```

### Backend Architecture

```
backend/
├── main.py         # FastAPI application
├── requirements.txt # Python dependencies
└── .env.example    # Environment variables template
```

## 🔒 Privacy & Security

- **No data storage**: Responses are analyzed in real-time and not stored
- **Local processing**: Most AI models run locally (except OpenAI API)
- **Secure transmission**: HTTPS recommended for production
- **Microphone access**: Only used during active recording sessions

## 🚀 Deployment

### Frontend (Vercel)

1. **Connect your GitHub repository** to Vercel
2. **Set build command**: `npm run build`
3. **Set output directory**: `build`
4. **Deploy automatically** on push to main branch

### Backend (Render/Railway)

1. **Create new web service** from GitHub repository
2. **Set build command**: `pip install -r requirements.txt`
3. **Set start command**: `python main.py`
4. **Add environment variables** (OpenAI API key)
5. **Update CORS origins** in main.py for production URL

## 📊 Performance Metrics

### Target Metrics
- **Response Time**: < 2 seconds for analysis
- **Accuracy**: 85%+ correlation with human evaluators
- **Uptime**: 99.9% availability
- **User Engagement**: Average session > 10 minutes

### Optimization Tips
- Use GPU acceleration for model inference
- Implement model caching and batching
- Optimize frontend bundle size
- Use CDN for static assets

## 🤝 Contributing

1. **Fork the repository**
2. **Create feature branch**: `git checkout -b feature/amazing-feature`
3. **Commit changes**: `git commit -m 'Add amazing feature'`
4. **Push to branch**: `git push origin feature/amazing-feature`
5. **Open Pull Request**

## 📝 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 🆘 Troubleshooting

### Common Issues

**Microphone not working**
- Ensure browser permissions are granted
- Use HTTPS in production
- Check browser compatibility (Chrome/Edge/Safari recommended)

**Backend connection failed**
- Verify backend is running on port 8000
- Check CORS settings in main.py
- Ensure no firewall blocking connections

**Model loading errors**
- Verify all dependencies are installed
- Check available disk space (models are large)
- Ensure stable internet connection for downloads

**Poor analysis accuracy**
- Speak clearly and avoid background noise
- Use quality microphone when possible
- Ensure good internet connection for API calls

### Browser Compatibility

| Browser | Speech Recognition | Audio Recording | Overall Support |
|---------|-------------------|-----------------|-----------------|
| Chrome  | ✅ Full          | ✅ Full         | ✅ Recommended  |
| Edge    | ✅ Full          | ✅ Full         | ✅ Recommended  |
| Safari  | ✅ Full          | ✅ Full         | ✅ Supported    |
| Firefox | ❌ Limited       | ✅ Full         | ⚠️ Partial      |

## 📞 Support

For questions, issues, or contributions:

- **GitHub Issues**: [Create an issue](https://github.com/your-repo/issues)
- **Documentation**: Check this README and code comments
- **Community**: Join our discussions in GitHub Discussions

---

**Built with ❤️ for better interview preparation**
